## CPU cache mapping (aliasing)

### 1. fully associative 
- every mem-block can be in every cache-line

```
|______32 bits________|
|_26b_tag_|_6b_offset_|
```

When data are cached in cache-line, corresponding tag is stored for addressing purposes
When data are being retrieved, tag of data being retrieved needs to be compared against every cache-line, 
implying O(n) time complexity, where n is the number of cache-lines

### 2. direct mapping
- for each mem-block there is exactly 1 cache-line 
- example: cache 32kB => 512 64B cache-lines


|  | 32 bits |  |
-|-|-
17b tag | 9b index |  6b offset


Cache is an array of cache-lines without associated tags.
Index is basically direct pointer into the array.
Given address therefore has exactly 1 place where it can be stored, so no cache-strategy is needed

### 3. set-associative mapping 
- every block can be in a given set of $2^i$ cache-lines


| | 32 bits | |
-|-|-
 $(17+i)b$ tag | $(9-i)b$ index |  $6b$ offset

Cache is an array of $2^{9-i}$ sets of cache-lines, eache set is identified by the index and has cardinality $2^i$,
so for every retrieval only $2^i$ tag comparisons need to be made.

## Memory hierarchy models
### External memory model
- CPU <-> Internal Memory (block size B, total M bytes) <-> External Memory (block size B)

Algorithm (CPU) is in charge of data transfer between external and inernal memory

**Def:** Alg. has I/O complexity $f(n, B, M)$ if $\forall B, M$ $\forall inputs$ of size $n$ it makes atmost $f(n, B, M)$  I/O operations.

Note: typically, $\#writes = O(\#reads)$ (if $|output| = O(|intput|)$)

#### Cache-aware model
- CPU <-> Cache (B, M) <-> Memory (B)
- Algorithm knows B, M but isn't in charge of data transfer between cache and memory


#### Cache-oblivious model
- CPU <-> Cache (B, M) <-> Memory (B)
- Alg. doesn't know B, M (Suppose $M = \Omega(B^2$ - "thin" cache) => larger overhead, good on hierarchical caches


### Assumptions (for cache models)
- controller has optimal strategy (offline - knows sequence of I/O block in advandce)
    - "throw out cache-line that won't be needed for the longest time"
- cache is fully associative


#### **Example:** Sequential array-walk
- aligned array with N elements, need only 1 cache-line => I/O complexity = $N/B$    
- Cache-aware: knows B
    - I/O complexity = #reads following optimal strategy (we don't know, something may be already cached) <= $N/B$
- Cache-oblivious: we don't know B, don't know the alignment => N/B + 1
=> In all models, $O(N/B + 1)$ IOs

#### **Example:** Mergesort
- 1 merge: 3x array literations => suffice 3 cache-lines
    - time: $O(n1 + n2)$
    - I/O: $O((n1 + n2) / B + 1)$

- $\lceil log N \rceil$ phases
- 1 phase: **time**: $\theta(N)$, **I/O**: $O(N/B + 1)$ 
- MergeSort: **time**: $O(N log N)$,             
    - | N | I/O |
      |-|-|
      | $N \geq B$ | $O((N/B) log N + log N)$ |
      | $N < B$ | $O((N/B) log N + log N)$ |

#### **Example:** K-way Mergesort
- $\lceil log_k N \rceil$ phases = $\lceil \frac{log N}{log (k)} \rceil$
- **time complexity**:
    - 1 phase: $O(N log (k))$
    - total: $O(N log (k) * \frac{log N}{log (k)}) = O(N log N)$
- **I/O complexity** (sufficiently large cache)
    - 1 merge: $O(\sum n_i / B )$  data transfers
    - 1 phase: $O(N/B + 1)$ data transfers
    - total: $O(\frac{N}{B} \frac{log N}{log k} + 1)$

- I/O model + cache-aware:
    - choose $k = \lfloor M / 2B \rfloor$ => $O(N/B \cdot \frac{log N}{log M/B} + 1)$