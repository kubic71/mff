# Will AI be ever be able to understand the world by itself? Team YES

## Debate strategy
- nothing specific, only vaguely
- no fundamental reason why it cannot happen in principle
- There is some reaearch done today that already looks promising
- Bobes is tired


## Counter-arguments
- AI  
- consciousness is a predisposition of understanding
    - they should provide definition

## Consciousness
- here we list out possible definitions of consciousness

### IIT (Intrinsic causal power)
IIT defines consciousness as a function of something called **intrinsic causal power**. The theory based around this notion is **Integrated information theory** (IIT)

The **intrisic causal power** $\Phi$ is a mathematical measure which we can use to compute amounts of **consciousness** human brain or computer posesses.

It turns out, that the intrinsic causal power is highly dependent on the type of hardware running the computation and less dependent on the software.

Current silicon computers have almost zero $\Phi$. Futhermore, the $\Phi$ doesn't depent on the software running on the computer. Imagine the future, where a powerful computer can run reasonbly precise simulation of the whole human brain. According to IIT, this human zombie brain simulation would be just as conscious as solving systems of linear equations.

So according to this definition and measure of consciousness, the human-level-intelligent brain-simulation wouldn't be conscious, but would still satisfy the requirement for the understanding.


### Conscious self-awareness, reflection, self-evaluation
- **this is metacognition, not really a consciousness**
- but if we accept this definition of consciousness, then the AI definitely **CAN** be conscious in this way

### [Agent57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)
-  deep reinforcement learning agent to obtain a score that is above the  human baseline on all 57 Atari 2600 games
- combines an algorithm for efficient exploration with a **meta-controller** that adapts the exploration and long vs. short-term behaviour of the agent
- **meta-controller**
    - responsible for efficient exploration, in other words efficient world exploration and understanding
    - intrinsic motivation
    - curiosity
    - adresses the problem of human task-assignment


## Interaction with world
AI needs to interact with the part of the world it wants to understand. Interaction is cruicial for discerning correlational and causal relationships. It basically needs to be able to test its own hypothesesis.

If it wants to understand humans, it must be able to interact and communicate with real people. 

If it wants to build rockets, it needs needs to understand material science and interact with physical things, metals, alloys. It should have a good understanding of basic chemistry and physics.

Much of scientific knowledge (physics, chemistry) is contained in the scientific literature so the AI could bootstrap itself from there, but in the end, it would need to perform physical experiments in the real world.

### AI real-world interface 

#### Interfacing robots
We can create AI with some actuators or robots that can interact with the world even if in only limited ways. It's easy to image, that the AI would have multiple different specialized robots (or interfaces) for each field it would want to understand.

Futhermore, the AI could leverage the current industrial infracturucture and tools, which the AI could use to conduct experiments.


#### Interfacing with humans
Internet goes a long way. There are people, which cannot hear or see from the moment they're born, but they ar still able to learn very abstract things.

We would let the AI play in the online space, chatting with humans, taking part in reddit discussions, even making video calls with them.


#### Reusing knowledge across AI specialty modules
- humans - only 100 year for learning, only ~50 year productive
- AI - virtually un-limited



(sensors, speech, mechanical body,...)









## Current AI is not able to generalize
- We should not limit ourselves to the current state of deep learning etc.
- Possible ways to reach generalization:
    - Try to come up with appropriate reinforcement-learning like system with intrinsic motivation so it explores the world itself. Provide a framework (like humans got a brain to start with) for this AI so it can define and modify its own representation according to its needs. This AI would be able to create necessary abstractions about the world and use them to solve an arbitrary problem.
    - Try to replicate brain-like system/simulate a brain at some level


## Do we need a superhuman intelligence or is AI on the level of e.g. dogs sufficient?
- One may argue that dogs (or even a lower level species) do understand a world even if it is  a world very different to the one we percieve, and possibly very limited.
- There is an iniciative called openworm of simulating a worm that lives in a soil Caenorhabditis elegans (C. elegans). It is one of the best studied species, we know basically about all the celly in its body and about all the neurons, about its genome and develoment. It has about 320 neurons and can sense and interact with the world, reproduce. They try to simulate biological phonemena (neurons and muscles) not only predicting what the worm may do. And it's actually working and it's a first step to understanding more complex biological systems such as human brain. They even made a robot with the worm simulation.


## Is brain simulation possible?

## Many AI researchers think that Artificial general intelligence is coming this century
https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/
Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with "never" when asked the same question.


## Possible explanations for the slow progress of AI research

## David Intro
All of us probably heard of the first AI winter, the second AI winter. Artificial inteligence was used only to solve toy problems and it seemed that it is impossible to use in practice. And here we are now in 2021 with both traditional AI and deep learning and machine learning solving real problems that we can't solve an other way. That was not even dreamed of in 1980s.

The current state-of-the-art AI systems have obvious limitations in understanding of the world and they may have narrow usages. Most of them require human help to work. Now it seems that the natural next step is to have some smart system that would understand the world on its own with no human interference.

I see many ways of how this could be achieved. Either via simulation or replication of a biological phenomena, that is neural systems or by a smart system utilizing technology similar to reinforcement learning. As I currently am trying to do a bit of research in computational neuroscience myself, I can see how much progress is happening in this field. It is a long way to understanding the human brain as a whole but a lot have been actually done. For example, there is an initiative called OpenWorm, that is simulating a small worm called C. Elegans on the level of individual cells. The organism has 302 neurons and about 1000 cells in total. That is nowhere near the number of cells in a human brain, however it shows that such simulations can be done and it seems like the necessary baby steps to something bigger. Then there is a kot of effort done in the research of spiking neural networks that try to simulate neurons and their connections on the level of spikes.

In my opinion, the question isn't whether is this possible, but when is it going to happen. There were actually a lot of surveys among AI researchers throughout the years and the majority of researchers always said the artificial general intelligence (which is basically an AI with similar cognitive capacity to human) is going to happen during this century. But I see, scientists are often optimistic in these things, but even if not in this century, it is eventually going to happen.


## Kubikovo intro
- will be quite brief :smile_cat: 

AI has went through several periouds of initial enthusiasm and optimism followed by subsequent setbacks.

We are living through the optimistic phase, mainly due to the recent success of Deep learning.

This has led many people to suggest that the singularity is near.

Even though I don't think it's going to happen any time soon, I don't see any fundamental reason, why it shouldn't be possible.

### What will be needed
- more general architectures
    - for example Neural-network models "reason" only for a few steps, given by their number of layers
        - exception: RNN, Neural Turing-Machines
            - but hard to train with Gradient descent
    - current models are "flat", not very hierarchical
            
- better, more general algorithms
    - Gradient descent cannot be the best optimization and learning algorithm
   
- focus on end-2-end, AI learning on its own
    - rule-based systems rely on human rule input
    - autonomous discovery of efficient representation structures
    
    
### Early sings of progress being made
- Reinforcement learning
    - surge of hiearchical reinforcement learning
        - submodules, better generalization
    - intrinsic motivation reinforcement learning
        - effective exploration, no need for human to design reward function
- David mentioned simulation
         
- but more breakthroughs will be definitely needed
