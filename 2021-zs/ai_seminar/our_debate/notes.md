# Will AI be ever be able to understand the world by itself?

## What does the question even mean?

### Understand
- compress
    - intelligence can be thought of as compression efficiency
    - world is fuzzy -> mostly lossy compression
- discover
    - find efficient representation and structure of the environment
    - find suitable abstractions 
        - abstraction is just general principle, capturing a lot with a little
        - boosts compression efficiency
    - analogies
        - find relationships, connect things together

- predict
    - understanding includes being able to tell what happens next

- would it be possible to understand the world without interacting with it?
    - can understanding be acquired just from observation?
        - AI couldn't "test" it's hypotheses
    - agent interactions are related to planning, motivations, goals and performance
    - seems like the ability to interact is essential for learning and understanding
        - at least Humans learn primarily in this way
        - at the very least, interacting speeds up learning significantly

### The world
- events and processes happening (mainly) on Earth
- the world of humans
    - individual human psychology, understand the thoughts, motivations and abstractions of humans
    - group dynamics, sociology, politics, civilization
    

### By itself
- end-to-end
- intrinsically motivated learning
    - unsupervised
    - no extrinsic task assignment by a human
- self-improving


## [Why AI can't solve unknown problems?](https://venturebeat.com/2021/04/02/why-ai-cant-solve-unknown-problems/)

### Current AI
- current "AI" are "plagiarists" with a layer of abstraction
    - GPT-3
- Deep learning said to be representation-agnostic, supposedly able to "discover" the representation by itself
    - but not true
    - the network structure fixed by the creator
    - the *intelligence* and problem structure contained in the specific neural-network model (CNN, Transformer, RNN, ...)

- DL is successful, because we have explored and cleverly designed models, optimization algorithms and curated datasets with good priors for each task
    - [Deep Image Prior](https://dmitryulyanov.github.io/deep_image_prior)

### AGI

AGI needs to be able to tell what is a problem and what is not, discover problems on its own. Currently the task assignment is decided by the researcher (classify images, model language, play Dota 2)

> Intelligent people can recognize the existence of a problem, define its nature, and represent it. They can recognize where knowledge is lacking and work to obtain that knowledge. Although intelligent people benefit from structured instructions, they are also capable of seeking out their own sources of information.

This sounds like a need for a metacognition. Ability to think about it's own thought-proccess and discover it's own deficiencies.

It also kinda sounds like this ability to turn the thinking machine onto oneself and self-reflect is a significant part of consciousness, but it's probably not the whole story.

Consciousness is mainly about experience, feelings and sensations.

One can be on halucinogenic drugs like DMT, which totally obliterates the brain's capacity to reason and to form and hold thoughts in head, but crucially, the conscious experience is still there. This suggests that the understanding and reasoning process doesn't need to be conscious.

We might be able to create super-human AGI, without it ever being conscious.


## [Will machines ever become conscious?](https://www.scientificamerican.com/article/will-machines-ever-become-conscious/)

The author argues, that consciousness is a function of something called **intrinsic causal power**. The theory based around this notion is **Integrated information theory** (IIT)

The **intrisic causal power** $\Phi$ is a mathematical measure which we can use to compute amounts of **consciousness** human brain or computer posesses.

It turns out, that the intrinsic causal power is highly dependent on the type of hardware running the computation and less dependent on the software.

Current silicon computers have almost zero $\Phi$. Futhermore, the $\Phi$ doesn't depent on the software running on the computer. Imagine the future, where a powerful computer can run reasonbly precise simulation of the whole human brain. According to IIT, this human zombie brain simulation would be just as conscious as solving systems of linear equations.

So uploading human brains to the cloud maybe isn't going to happen.

## [Why general artificial intelligence will not be realized](nature.com/articles/s41599-020-0494-4)



### Definitions
- **strong AI** - AI in principle identical to human intelligence
- **ANI** - artificial narrow intelligence
- **AGI** - artificial general intelligence
    - its distinguishing feature is generality (like human intelligence), but doesn't have to necessarily be a clone of human intelligence
    - human-like, not identical to human intelligence

The article refers to the distinction of ANI an AGI.


### Hubert Dreyfus argument
    - human knowledge is partly tacit, and therefore cannot be articulated and incorporated in a computer program

### Main thesis
AGI cannot in principle be realized, because understanding means understanding causal relationships, not merely computing corellations. 
Causal knowledge is an important part of humanlike intelligence, and that computers cannot handle causality because they cannot interact and intervene in the world.

### Anti-thesis
We can make AI agents interact through some interface.
AI agents can learn from the Internet and talking to people online, taking part in online discussions etc.
How limiting would be only linguistic interaction? What other interface sensors and actuators would be useful?
Is digital interface enough in principle, or do we need to add more interfacing capabilities?
Can we make an AI, that would come up with new interface requests on its own?

