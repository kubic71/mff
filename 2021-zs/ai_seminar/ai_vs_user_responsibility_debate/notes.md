## Yes
- user should know AI isn't 100% precise
    - if the user willingly takes the risk, he should be held responsible
        - negligence

## No 
- AI has been certified, properly tested
    - strict, limited scenario
    - Doctor using AI-driven medical imaging
    - User using AI product, actions result from automated reasoning
- 3 pillars
    - complexity
        - should we blame users, if they cannot see and understand the inner workings of AI
        - AI engineers and developpers responsible for creating trustworthy AI and guidelines
        - as far as user follows the guidelines, he should have immunity
    - regulations for trustworthy AI


Yes -> No
No: Developpers should be more responsible.
If mistake happens, we don't have adequate regulatory framework, that's where we should pour our energy.
When all the regulations and requirements are being followed, the user shouldn't be blamed.

If user of level 3 selfdriving car doesn't pay attention when he is required to, he is of course responsible.
But if Level-5 selfdriving AI crashes, then by definition the user isn't legally required to takeover and shouldn't be held responsible.

### Burden of proof
- Is user required to proove that he used the AI correctly, following all the guidelines?
    - Or is the manufacturer required to provide the evidence, that user misused the AI
- Manufacturer 
